{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syt6ssVGOqA_"
      },
      "source": [
        "## Final Code with deatils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgFsuFJWOpqM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOeVksrBMGnR"
      },
      "source": [
        "## HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6oMtZhLObxF"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "\n",
        "max_iters = 5000  # to be tuned later\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "head_size = 16\n",
        "dropout = 0.2\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqAvDjyoMLDC"
      },
      "source": [
        "## Dataset\n",
        "We're using the same Shakespere dataset, however A larger dataset such as [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/) can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQFrFx1XPBEq",
        "outputId": "f928ee37-9c5a-42b1-dd10-cb4cbcc2d458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 20:02:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-11-30 20:02:09 (25.7 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3MDs6AHuc1I"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"Skylion007/openwebtext\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5FA0OlgPEsc"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = ['\\n',' ', '!', '$', '&', '\\'', ',', '-', '.', '3', ':', ';', '?', 'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "vocab_size = len(chars)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZsEIaooPWtm"
      },
      "source": [
        "### **Encoding/Decoding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge5bl9U9PLdM"
      },
      "outputs": [],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6_tHxvvPaqf"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZthA4VvsPqG2"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqKp_yDdPmeC"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp21IOIjQjuY"
      },
      "source": [
        "### Self Attention (with RoPE)\n",
        "Rotary Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYZFHsZAC16j",
        "outputId": "b989ac9a-abae-4352-c2a3-d1ea27908910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (0.7.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->rotary-embedding-torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->rotary-embedding-torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rotary-embedding-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usA-Xv9eCvVk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from rotary_embedding_torch import RotaryEmbedding\n",
        "\n",
        "# rotary_emb = RotaryEmbedding(dim = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnvlKeK1QnHe"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size, RoPE=False):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rope = RoPE\n",
        "        if RoPE:\n",
        "          self.rotary_emb = RotaryEmbedding(dim=head_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # Applying rotary ----------------------\n",
        "        if self.rope:\n",
        "            q = self.rotary_emb.rotate_queries_or_keys(q)\n",
        "            k = self.rotary_emb.rotate_queries_or_keys(k)\n",
        "        # --------------------------------------\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZNIIb0TRXfw"
      },
      "source": [
        "### GPT Model with both token and position embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LeqYgMlRVxk"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC2wdz1gRwuU"
      },
      "source": [
        "### Running and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9e3CDY2Ruka",
        "outputId": "cefa3f6f-0fec-489e-8115-5203e6ec6a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.3331, val loss 4.3317\n",
            "step 500: train loss 2.0315, val loss 2.1073\n",
            "step 1000: train loss 1.6151, val loss 1.7852\n",
            "step 1500: train loss 1.4533, val loss 1.6504\n",
            "step 2000: train loss 1.3522, val loss 1.5616\n",
            "step 2500: train loss 1.2832, val loss 1.5238\n",
            "step 3000: train loss 1.2356, val loss 1.5053\n",
            "step 3500: train loss 1.1899, val loss 1.4925\n",
            "step 4000: train loss 1.1564, val loss 1.4766\n",
            "step 4500: train loss 1.1183, val loss 1.4763\n",
            "step 4999: train loss 1.0849, val loss 1.4788\n"
          ]
        }
      ],
      "source": [
        "model = GPT()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# Using ADAM optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluating the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ll4j5vJR-Cw",
        "outputId": "f322e288-37de-48a4-b2c8-dbf4f42e65cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Then, grave me my lord's uncle;\n",
            "I never true Gremio's dauble king make make his wrongly\n",
            "Is this my way: a playforment stretched,\n",
            "What to Walesby, poisoner to England;\n",
            "Spreved his father Richard lies kill dispraughter.\n",
            "The orfer is the nor Edward, and come\n",
            "To Raves Nortespirah?\n",
            "My Lord's rest Christsengthman: be defiged\n",
            "Tim haply but promised to the motheror;\n",
            "Thus had stew us I losen to thy stains.\n",
            "Death, there he canst thou a bride!\n",
            "\n",
            "BENTIO:\n",
            "Take hee them.\n",
            "\n",
            "BISHOP OF WARWISLOUCESTER:\n",
            "That you, friend me not play to quicker.\n",
            "\n",
            "QUEEN:\n",
            "Would told Come hither weary well, Claesby,\n",
            "Her robes and the England comest counselected their affairs.\n",
            "The venom heavy manners proceeance in joy,\n",
            "I am hairy to breach and Montague.\n",
            "\n",
            "Lord Mayor:\n",
            "What's that, are not yet to lent the gentlay tame,\n",
            "They comes the tenderous place and traves.\n",
            "\n",
            "Second Murderer:\n",
            "To exerer.\n",
            "\n",
            "AEdile:\n",
            "\n",
            "Civenture, Clifford:\n",
            "You better son, the first uphurp of herself:\n",
            "He shall be seasonly loves.\n",
            "We may not arm, event may do met grasssing tongue!\n",
            "Shouson our most filly baccastagers with bleft\n",
            "your beguile and her fire anger is gentleman:\n",
            "Grandam, me that is yours honour' sweet with her,\n",
            "Who raw the hestiery dam prince after,\n",
            "Which or to faith had madness with women, by lie,\n",
            "Who in this court life of caunting stape-haste,\n",
            "Bid so keep in balding great eye of sapiter;\n",
            "And all the banky refless of your gries:\n",
            "Yet thieformadam, conduptain from them design,\n",
            "And much assured on.\n",
            "They canst be to the balm\n",
            "Of the silent wife of jest each intext\n",
            "Enountering 'montage\n",
            "And such castly's dagger for brain,\n",
            "His bosom's father want want to the Valoc.\n",
            "\n",
            "CORIOLANUS:\n",
            "O, what o?\n",
            "\n",
            "MISTONUS:\n",
            "If he is this!\n",
            "\n",
            "COMINIUS:\n",
            "It was both tense in thy teach hangmas?\n",
            "\n",
            "MENENIUS:\n",
            "Good noble; your besteen lives upon fight;\n",
            "And his lips in one wars that Pobbon's blace.\n",
            "HASTINGS:\n",
            "To, though I dityrancel parts, shalt be formed,\n",
            "I may look as too holy house\n",
            "The deament; that will be muckers with the sister palt,\n",
            "To mountain another much most increase tauntion,\n",
            "Which bite have honours so, see't, to make you\n",
            "Would not take paintorOenius, or old to the heart.\n",
            "When the kingdod shall this in waxen briden on yourself\n",
            "There small gave you to this.\n",
            "\n",
            "SICINIUS:\n",
            "We will bear thee, braggar Bohemia and world!\n",
            "There is new new to be deach on ten through again.\n",
            "O Marcius Marcius is Marcius Calais,\n",
            "Deserving in almost about not exedenT,\n",
            "When thou not with his death.\n",
            "\n",
            "MENENIUS:\n",
            "What is't closely to the Fourth?\n",
            "\n",
            "CORIOLANUS:\n",
            "Because thus, the art comendities.\n",
            "\n",
            "MENENIUS:\n",
            "Elder, cousin! There is can, true Volsces.\n",
            "\n",
            "Girloran:\n",
            "Roman:\n",
            "Our perfector! did you further?\n",
            "\n",
            "SICINIUS:\n",
            "It is't in my hand; this is the work graced enceive:\n",
            "Tyrrel i' the hear, and these tonigher gives sort\n",
            "That one thy sixteen Romeo wert thy pillige.\n",
            "Divineter shout that still I see thee to bed,\n",
            "As that little to by substricts us vow,\n",
            "And breath aintic little adves thy news!\n",
            "\n",
            "FRIAR EDWARD:\n",
            "No, God Juliet posit, with thy onfury a story,\n",
            "To witnesship'd the wife of one again;\n",
            "For shhall I did remute vaintory to me,\n",
            "Their lavy passage of the time years,\n",
            "Than mine, I procured lost.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Curse beforce Richard doth close from a wounds;\n",
            "And I will setty impedimation as they quickly\n",
            "So may together, if the proclaiment councilenature of man\n",
            "More name who wrong'd yon these yoke I lray accedance\n",
            "Your heasted chanks, add smiles amiercy,\n",
            "But you faithful on ply; and fixed the whis\n",
            "Would call dequite him the custom of commonwealth\n",
            "Of would his soul rest now a children? ah,\n",
            "That, as he can be ever\n",
            "To see this? That that has the effection was buse a\n",
            "dainted on of my garmentation; his beauty, heady as\n",
            "Proceeded their courses true servate.\n",
            "Your queen is right run to my stone\n",
            "That less of yet that palses were of slubscrified.\n",
            "Vilian ascend with happies, they lives on.\n",
            "And this deerneath of my mistheery seaths,\n",
            "Should Aufidius digents was in earth\n",
            "Of betweet offixed my sons in powdering;\n",
            "Standing shall up ill require kill the queen.\n",
            "I'll speak with that he makes in time of scave,\n",
            "Edward in that rights reply fought.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Hail; in't: and he best that died in you Forth.\n",
            "\n",
            "DUCHESTER:\n",
            "But in that opinions lay augh he still the king,\n",
            "And trusted gnature to his sweet me scarcifUS.\n",
            "\n",
            "CAMILLO:\n",
            "What, God lord?\n",
            "\n",
            "POLIXENES:\n",
            "Then hart the dailifull Antizen, are virgin,\n",
            "Save as the worldly should become to Ravens.\n",
            "Be prel Claudio, for the queen instruction;\n",
            "For Lord Wastinghter lived, have pawn'd to Romeo,\n",
            "That your gracious love bear the pride,\n",
            "Are not a head in joy triumpht for you;\n",
            "And so ang sily not one in humble embassage,\n",
            "I fear your brandage; your was drinage largectory,\n",
            "Whilst well not stay, ruches lontested resting me.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Yet all your life and leads us rideem'd,\n",
            "Who I will chan no prize very deeds,\n",
            "And still if yet the in Margaret come the\n",
            "Beauty's fancy are famouse banded.\n",
            "\n",
            "DUKE OF YORK:\n",
            "But now, what not mildly live another?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "My gracious degrace to little that was yet I\n",
            "Was not wonter'd so bad into so but thankful straitor\n",
            "And so late a Richarge terror.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I have revolthed nother come Isabelly.\n",
            "\n",
            "KING RICHARD III:\n",
            "Give me all ill: drue it be must most grimenant?\n",
            "\n",
            "EDWARD:\n",
            "We are now fell'd follows it to take off their hearts.\n",
            "\n",
            "RICHMOND:\n",
            "Ohrow our thoughts I won she give upon alter'd.\n",
            "\n",
            "GLOUCESTER:\n",
            "Montague, when thy woman maids.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Well, what thinkes I kill joy, there's love?\n",
            "\n",
            "POMPEY:\n",
            "Coventry, where he?\n",
            "\n",
            "BISHOP POLIND:\n",
            "Tut, though to Else now a woman of thy babe,\n",
            "I may yet be my brother curses home against thee;\n",
            "Bid Clarence!\n",
            "\n",
            "DION:\n",
            "Madam, hark again, and wake him waked!\n",
            "Thine that I love the more of his recore and\n",
            "A fielding to Laurence hath a man?  make ye not shad;\n",
            "For seekles how his parted wills to the seal things:\n",
            "I need that had I sent for from him! A proimed author.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "I know that, I am let my wash;\n",
            "Nor pelted my husband, it will pity have me,\n",
            "Howearing with inn my heart to ruove town,\n",
            "In alainted Edward's wabilice and sun:\n",
            "But shall jealous is past, be go in sleeping, cannot\n",
            "Covery and where that Clarus, good love; there\n",
            "I'll see they will him and wean see, stay me down.\n",
            "\n",
            "HORTENCE:\n",
            "Amen, madam, aom I can shorten'd in him;\n",
            "And provise his king brother purious,\n",
            "Which after way this want with his task'd drovering\n",
            "And hath he seen'd swepted the thing.\n",
            "Have ofter'd Clarence? ark! it seizen strait thee?\n",
            "There lords will to-lord-Serving corow? Or ohes ir angries\n",
            "You, five Clifford, and found is delight?\n",
            "And unatesy-shied 'Halp ruin your goods,\n",
            "Courage occabsta's someth holloworn that you bid!\n",
            "All for the king dioubt of arms,\n",
            "Whose and rain are bank camainting of youth?\n",
            "Is offering any thing Richard the fair after,\n",
            "Thou wouldst must be foul so well at needs,\n",
            "To whose assauce a broad father, cast morry,\n",
            "Despair my partners, most; tween to the weather,\n",
            "Haplots in the kingdom or of my childity:\n",
            "My Lord of Green and thy fair cousin Henry,\n",
            "And methink my curse slips:\n",
            "Yea, obeying towns, 'tis a fight, undoth mine,\n",
            "May not be a made require proped to the king;\n",
            "And that if you be no suit worse, may consent,\n",
            "Consent laid, what say burst die for which shall he respect\n",
            "Those Onnious to this infite our dish babils,\n",
            "Confess me breathe in all: ten my holy drice\n",
            "I'll not bear us bruising to: look thou noble death:\n",
            "Witlot and Softle honour cand Such crotces and such detes\n",
            "Upond Exed by view sevenocatory, hast thou.\n",
            "O Juliet! whom thou woe'st Turower?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Though be owed?\n",
            "\n",
            "POMPEY:\n",
            "A care your sensisterity, I have fall'd more more time.\n",
            "A gentleman revenge, and say no more\n",
            "Than lal I cannot be punish'd wheteously\n",
            "When in the strange forfeining-scuade.\n",
            "Displease his news breast, Waterping incerse.\n",
            "\n",
            "CAPULET:\n",
            "But he now, what can you say.\n",
            "Could hencitive his honours bands and most signnity,\n",
            "Who is this placently day is my eyes gains?\n",
            "Are thou worth thy heart thunkindry in thy life.\n",
            "3 HENRY BOLINGBROKE:\n",
            "No, nor all injoy come,--\n",
            "\n",
            "GEORY:\n",
            "Norious are consecution?\n",
            "\n",
            "ANGELO:\n",
            "No, in him his countraction life accuses\n",
            "I could have I desire true turn love in death.\n",
            "Am I that can after it be so, that cuthance\n",
            "The windy shallow grows fools! O trusty pain!\n",
            "What was the leasing were onne for pierce\n",
            "Did since my swift death ill reproof! But, tell me\n",
            "wratches o' the body of the wivest slaughter-bord in\n",
            "And fantainticle forcely on air,--O, foe to be ghoste!\n",
            "\n",
            "MERCUTIO:\n",
            "Hear you!\n",
            "Your mouth is heaven slues Romeo\n",
            "And blunt so bond that this villainly earth.\n",
            "These sheep comes to the cient, the soverey in hange;\n",
            "With conscient dependants, youcge music, where to-morrow,\n",
            "Because to heaven after, accidence bring and be right,\n",
            "And provoked my oath; it is tide's care, give Richard\n",
            "\n",
            "BURY:\n",
            "I am none; to see there, obstain alone in the youngest,\n",
            "Who never spake his rock envying: so say you\n",
            "To-morrow Paris; he is Richand before the grace\n",
            "To make the anger fellows accidence against sack\n",
            "Jesused not and his gain\n",
            "So that the table abstage of ho mocketh.\n",
            "\n",
            "BENVOLIO:\n",
            "Now, sir.\n",
            "When good prayer, at safety Paris,\n",
            "To modestre Play forth that be lurking'd\n",
            "Is to say God, she'll she stool'd. Yeal, O side,\n",
            "And, I fear thee yields the rushiness men:\n",
            "For who cands but sender to the past that bring of\n",
            "The fighteous kindred man's haste? I was not with\n",
            "sand an every fear?\n",
            "\n",
            "SICINIUS:\n",
            "No near no nor woman.\n",
            "\n",
            "BRUTUS:\n",
            "What is Judains?\n",
            "\n",
            "MENENIUS:\n",
            "Gracious noise of fatals nature?\n",
            "There certailyt dies the bluise to slaunt,\n",
            "But King Richard and with her bearts in mend.\n",
            "How now shall quickly in the edge excuse,\n",
            "That Mowbray to Friar Mabus, the stain of Bishop\n",
            "Are to dust as dire boldier'd inceasies? Isabel,\n",
            "The desperation--\n",
            "\n",
            "SAINLEY:\n",
            "This good princes, whose children to thy royalm?\n",
            "\n",
            "Post:\n",
            "I'll the king offence more son:\n",
            "That will you have from thine forfeit that are\n",
            "Will die your estills. But, let me countern.\n",
            "\n",
            "RICHMOND:\n",
            "Ay, good Sir John, makes a show of this tear.\n",
            "Thy Yoursea lady: cheerly a fair battle, hell--\n",
            "While comforted Henry's younged Night.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Ay, but I not only live,\n",
            "End coin Bishman a present man even \n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running DDP (Distributed Data Parallel)\n",
        "```(code will only work on multiple GPUs)```"
      ],
      "metadata": {
        "id": "EaW27gJxnp5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "\n",
        "    # initialize the process group\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "\n",
        "def cleanup():\n",
        "  dist.destroy_process_group()"
      ],
      "metadata": {
        "id": "ILLOCmCzno7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(rank, world_size):\n",
        "  setup(rank, world_size)\n",
        "  print(f'RANK: {rank}')\n",
        "  model = GPT().to(rank)\n",
        "  ddp_model = DDP(model, device_ids=[rank], output_device=rank)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = ddp_model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  cleanup()\n",
        "\n",
        "def run_function(runner_fn, world_size):\n",
        "  mp.spawn(runner_fn, args=(world_size),\n",
        "           nprocs=world_size)"
      ],
      "metadata": {
        "id": "bGwMwST8nvTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "world_size = 1  # asssuming we have 1 GPUs\n",
        "run_function(training_loop, world_size)"
      ],
      "metadata": {
        "id": "KYuxKWRsny3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hKt7fsV3n0qa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}