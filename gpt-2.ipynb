{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syt6ssVGOqA_"
      },
      "source": [
        "## Final Code with deatils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WgFsuFJWOpqM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOeVksrBMGnR"
      },
      "source": [
        "## HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6oMtZhLObxF"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "\n",
        "max_iters = 500  # to be tuned later\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "head_size = 16\n",
        "dropout = 0.2\n",
        "# ------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqAvDjyoMLDC"
      },
      "source": [
        "## Dataset\n",
        "We're using the same Shakespere dataset, however A larger dataset such as [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/) can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQFrFx1XPBEq",
        "outputId": "f8273059-6814-4666-b384-181928b3cd21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-30 12:18:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.5’\n",
            "\n",
            "\rinput.txt.5           0%[                    ]       0  --.-KB/s               \rinput.txt.5         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-11-30 12:18:14 (18.7 MB/s) - ‘input.txt.5’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3MDs6AHuc1I"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"Skylion007/openwebtext\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5FA0OlgPEsc"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = ['\\n',' ', '!', '$', '&', '\\'', ',', '-', '.', '3', ':', ';', '?', 'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
        "vocab_size = len(chars)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZsEIaooPWtm"
      },
      "source": [
        "### **Encoding/Decoding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge5bl9U9PLdM"
      },
      "outputs": [],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6_tHxvvPaqf"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZthA4VvsPqG2"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqKp_yDdPmeC"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp21IOIjQjuY"
      },
      "source": [
        "### Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYZFHsZAC16j",
        "outputId": "152879eb-a76c-4587-9107-081ba6963e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.10/dist-packages (0.3.6)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (0.7.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from rotary-embedding-torch) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->rotary-embedding-torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->rotary-embedding-torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->rotary-embedding-torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rotary-embedding-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usA-Xv9eCvVk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from rotary_embedding_torch import RotaryEmbedding\n",
        "\n",
        "# rotary_emb = RotaryEmbedding(dim = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnvlKeK1QnHe"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rotary_emb = RotaryEmbedding(dim=head_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # Applying rotary ----------------------\n",
        "        # q = self.rotary_emb.rotate_queries_or_keys(q)\n",
        "        # k = self.rotary_emb.rotate_queries_or_keys(k)\n",
        "        # --------------------------------------\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZNIIb0TRXfw"
      },
      "source": [
        "### Bigram Model with both token and position embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LeqYgMlRVxk"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC2wdz1gRwuU"
      },
      "source": [
        "### Running and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9e3CDY2Ruka",
        "outputId": "ab60881c-7e0a-46a0-db70-993723e714ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.790081 M parameters\n",
            "step 0: train loss 4.2871, val loss 4.2971\n",
            "step 100: train loss 2.4736, val loss 2.4879\n",
            "step 200: train loss 2.4132, val loss 2.4382\n",
            "step 300: train loss 2.3225, val loss 2.3541\n",
            "step 400: train loss 2.1557, val loss 2.2155\n",
            "step 499: train loss 2.0229, val loss 2.1066\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# creating a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1Ll4j5vJR-Cw",
        "outputId": "f6aeffa4-dc07-4568-c736-6c4395adb8b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Add not nith mus ack'Winuine.\n",
            "\n",
            "LAURET:\n",
            "Arcers:\n",
            "What lad.\n",
            "\n",
            "SICAy, puspape:\n",
            "I PAysiour.\n",
            "GLed:\n",
            "Good Vovietest fant senfoorem:\n",
            "Whe sony wert,\n",
            "BucCll my wathilk bagan the ou her'd reve\n",
            "Prsshart, to me conotistere, maw praws ackencee,\n",
            "t nees bubot e if oullom\n",
            "Thiseinnd so it he dem pto&s tanty the stheled.\n",
            "Had at bend man sonthe. he wa her sthave thil:\n",
            "Duf comb'ds swell umy dolded.\n",
            "\n",
            "\n",
            "NOMABEENT:\n",
            "A blove a Los's bot mok gatio a dach\n",
            "Iiretencend beotto is I themout re yourd\n",
            "To buld that the mp tho sup dooo\n",
            "Ascht eats his ofondsor tof lon;\n",
            "I san Bye drastheque woran un tonf\n",
            "Prent an car lland ayo gearnd now ssthseas:\n",
            "What cand obnut hag this whate fame. Ed bay I\n",
            "Sanou souew no whe howrrch d misssot; I byought not.\n",
            "\n",
            "HeR sours my senes mod ang youg ofod orr witongnss trrepot.\n",
            "wherrs hiss the thave stho I sed meng;\n",
            "Noththe I thathe mo, tnot.\n",
            "'O, heer now toll yoordigs.\n",
            "\n",
            "\n",
            "POLAUTINENCE:\n",
            "Willd weeno o dellmande fild, feme.\n",
            "\n",
            "\n",
            "QUEE ETENTEw:\n",
            "A for meeneer.\n",
            "\n",
            "IRDIUTTEONT:\n",
            "NilEN hate we the sue, danhter word to-me au\n",
            "Bend wing hering I merong--he that faved mbar sofe f foor mes,\n",
            "dok treamendserderkes sumy grelse\n",
            "\n",
            "Thach annd slim, on hir peaurs for-mathch ared,\n",
            "And my sothan, dus achald wig! Edath and nom\n",
            "\n",
            "wathoby i th fres ea'd sheavys par Youllds whreas pricar compsun.\n",
            "\n",
            "\n",
            "Past:\n",
            "Thet his del pofe as buport'd wit bror mie Thaks,\n",
            "An bet goole me ther my head\n",
            "We Nof dour uthe the shis in lie,\n",
            "Ars balte lert binag trant le frcom sut.\n",
            "\n",
            "GRIOF YORK:\n",
            "If lou by my ind gune's muechld,\n",
            "\n",
            "SBourd your har saus usoures brnchom nowsse,\n",
            "Wich tinkengus as pente o bebrph\n",
            "Noghtth sheat'd, his in hist we: my yourecur himses eacNong'de?\n",
            "\n",
            "WOROK:\n",
            "Fothy jutesereren, and mare, oink andef,\n",
            "Fo doy theno sof ther, yeriony wheete grie,\n",
            "TI will beertnks on scacull cang aur he cos\n",
            "And neasiown seard hang dofasaret'ds pithirs oamer f hamPitare liand\n",
            "Tht sheave My rarlanks; heherd an?\n",
            "\n",
            "Bold conerer.\n",
            "\n",
            "DUCEY RINCENT:\n",
            "Hlakered maquse tho's lands groood? Muechid god Enom,\n",
            "We'll weat hoth s your hering sshe?\n",
            "\n",
            "Tilnce ler fer sont is mie.\n",
            "\n",
            "\n",
            "CSARIAND:\n",
            "I it; mast we your kit, forr nous hos as;\n",
            "Ar that co granchtt Kis eas sotin. Well gucot you,  wutwethtin tou\n",
            "Wheme you stourert seigudkens woldend dong,\n",
            "This goave oo bed wath oa weigh's Your id resbewited blf y mam mase,\n",
            "Leas begorlk; stor ay lie ikerde, dout.\n",
            "\n",
            "Thave durueng wh ind swons ming;\n",
            "Ind sthely and heas buseld? Go sto Cont ons mato cly pentee thes?\n",
            "\n",
            "DUKE MORT:\n",
            "Noir,\n",
            "Plldat heas't!\n",
            "\n",
            "\n",
            "CIUS:\n",
            "Ay incan, Wins yo gof is Carar's endy ar woarr;\n",
            "Whim cand the imysty sorght. Retnoce if,\n",
            "Thes loousio! is ayou wit therems and lave so ealld,\n",
            "Myorde pranube's lovechiar chat to he sand as tabun tiverte.\n",
            "\n",
            "Thir Lest fayes I wor soom.\n",
            "What my Macerthe ald-his mavo; natran?\n",
            "\n",
            "I's Math plean the I cany verand sharnure.\n",
            "\n",
            "BULIUS:\n",
            "Thar co you to so gothil, tof the,\n",
            "A the frir! ap, ois tithiew:\n",
            "Wengo surot chel to thals sowanoo, andirs!\n",
            "\n",
            "\n",
            "\n",
            "ROUCIO:\n",
            "Go fordd?\n",
            "\n",
            "OSCORYOR OF LONCENCE:\n",
            "No towretar, you this anter\n",
            "Wich thes int amelf apit ins omy in\n",
            "I ie whad me jud, mange anceshit nom'dding?\n",
            "\n",
            "\n",
            "YLUCER TVINARE\n",
            "\n",
            "OLAM:\n",
            "Ou uoum dor movery, ciIt mut Machichty, aur\n",
            "And am hat child be fey har,\n",
            "And mo, dery, and eambum, Nound oime.\n",
            "\n",
            "\n",
            "KING ING ENGHARD ED :\n",
            "Whay illf bede crmen yond that cone Lold wak;\n",
            "Thand salery fou sor suld com do congus we ith\n",
            "Thy leafffe mett, ber himne shemely bansured.\n",
            "Sis;'s I noy neem; thup niuge onc,\n",
            "We nerseghaserw camH. Offerme inow. my!\n",
            "Anath hee quioks yough condseakd toney.\n",
            "\n",
            "\n",
            "KING DRMIVIIZAS IOwn mavesun rust, hi' I, beathtld\n",
            "\n",
            "KI'll sis bet heave oim treanear my. Ooure Preshin:\n",
            "So as nowr noth menqug im.\n",
            "\n",
            "Werobld the sthe\n",
            "Ber genaied a tour sand mcor dobto shished\n",
            "And overeth, allak fof? Marend nost\n",
            "And bag ast.\n",
            "\n",
            "\n",
            "FERDY:\n",
            "Foo, mishe and Got mestue ham urded! vay but wowe greak!\n",
            "Butistuth dames lour of shist\n",
            "And the rieinghat, atht suctalfaus he muord, onds!\n",
            "\n",
            "MEELINCER:\n",
            "Whall uld oftchengerenkere, frearkermbr thand nithek thaghaure,\n",
            "Once frerum quee frleor, woonchthu:\n",
            "Rot thece the gon!\n",
            "\n",
            "SeENTES:\n",
            "Wer geainte, aldestion, hom is itht os neveaner.\n",
            "\n",
            "YOMNTEN ORY:\n",
            "Sheere yoftar woputer\n",
            "Why m cusor ablkess tof musch haram\n",
            "Thouldsby wi\n",
            "IS Maker of mue uncci?\n",
            "\n",
            "FLAND:\n",
            "Yor you tra lant'stblatiiengee, diabooor bly'd,\n",
            "Yourd ansur, repavad\n",
            "An re ontere cand meivent:\n",
            "Whike sar ta Gouted; I's fethar tof, the premeied.\n",
            "Glionde. vene Mutins arord, sooom of nourrck,\n",
            "astad to my. ge mye ean, with in lirngowave\n",
            "Hed rowo afe: gars Jutunk ar han.\n",
            "\n",
            "SLever sings have ener, teavell ery, egofoke,\n",
            "Nod nove osen\n",
            "Santirt soug omal coff coomays,\n",
            "Can:\n",
            "Thot ourt yall his! Tho, 'thaveat: beRok, sar,\n",
            "Wid inedulaives ele.\n",
            "\n",
            "QUEEN OF foway, Iistt meveeseerse sit thet\n",
            "Whir hoave sus andlt you odan wond ler,\n",
            "Duchacin mcher mornou dan foce.\n",
            "I annde wom wogh word ritath and there's map.\n",
            "Wewmer a aind.\n",
            "Rorat Sey ank frich thatheses--ar uby wokelll,\n",
            "The my love eand ma lldead he wont:\n",
            "Al me; tho tans, wiPlises knot fit sir hof'd wit\n",
            "Co sgrrasth, elus in inou g bard otor thenecich ue\n",
            "A somor rosse a sair monkesss'd of to prmaiat.\n",
            "\n",
            "DUMEN Iforge ofto, haman kinf, mott anresel!\n",
            "\n",
            "Had ho an lemorncom oas thin. My sorard geing fthie.\n",
            "What baverng whis mat;\n",
            "Evil swit's af anded lenced an, geof himlp!\n",
            "Lood tha thosd hird yas lot,\n",
            "Hem nest thare nor me tetor.\n",
            "\n",
            "ROMVARD:\n",
            "AP dishte:\n",
            "Cos my unacen:\n",
            "Wherr kse DUprot:\n",
            "I wawos Meerd--He Vourry.\n",
            "Showe Naw Of waverthe; lidsant,\n",
            "\n",
            "Iferer cung, gory sase pince sander his\n",
            "Thow ir cinlough and thas mether eand\n",
            "And puchenses non groo saieds ond\n",
            "Forld?'s Fir Evin'shts moren'd fomele.\n",
            "\n",
            "\n",
            "DUCHERD OK:\n",
            "Now an 'To'tthe be sthor henre cowthiencuat's, me orot;\n",
            "And eand wed thar for apenes duce,\n",
            "Esellatson all willd on; lershig sher yothed crour\n",
            "And may weth grand bura tay reard.\n",
            "\n",
            "YORDVOMy Yourrtel danarrbeny hevechig yores,\n",
            "The ill not wil and ustil, mirs.\n",
            "Ork conter frood loobllfe!? for.\n",
            "Sommpely titht mak's tof el meerdeponck bere\n",
            "Ank, and ther thims, a salld Reaknt be hatiaghe\n",
            "Who Hill emift hi sthis my loou denghe'\n",
            "Whitht vis gan maverd, fowh shee miste,-hine,\n",
            "And stast mink, oll himes my colldsee peast'd dead tru.\n",
            "\n",
            "SeMy hitly wirturde rith dif now. Lomes conand,\n",
            "Cometede.\n",
            "\n",
            "SADUSTHENXE:\n",
            "To, now in loake, we the murs.\n",
            "\n",
            "\n",
            "LOUCEONVERE:\n",
            "Mas nontl supened; you clldeie, noted Come, manbbtte.\n",
            "\n",
            "INull Ofe boffe, mir, you I bexis the lis mun it\n",
            "Thef sicherin coms lod nove wethe har she Peand,\n",
            "Favour rowe antie him blly sheald.\n",
            "\n",
            "Nor:\n",
            "That wot if rome prop; tist, you.\n",
            "Caler: myou.\n",
            "\n",
            "Butsich! Mondiures and\n",
            "If wor tha-mengay bus thert'l cofadshad sitelting tekebam.\n",
            "\n",
            "PLOP:\n",
            "Fe this And it swors lot of vordgod.\n",
            "\n",
            "\n",
            "SiYOMENTES:\n",
            "Co me se meshed uss achimay us thers?\n",
            "\n",
            "Ond fie hit, schand, macllley:\n",
            "What? Lixte pitle, anth, and Gidee andseir,\n",
            "This you Kip aveald coreres,\n",
            "Nur, onanch I, his noat the ryou hull\n",
            "On peatts the yo vourd done so ker;\n",
            "us vear and Letos me isond, butund fis dined'd vof of Ay leew\n",
            "Well  ither an theren hisde, all the ming roy do medzed.\n",
            "\n",
            "CINIUS:\n",
            "Why?\n",
            "\n",
            "\n",
            "FORES:\n",
            "Qu, gaid o the whack; now thon peag heat.\n",
            "\n",
            "AProoms LARAMd tirse mer ya hald beat delt\n",
            "Dio the hat rins besen wit knot; horey alloing,\n",
            "Thacht the alerm hist woch, der comeanklad,\n",
            "Thim cre, iun with: le thou mave,\n",
            "Ind theh mark dinth o me tey,-oulld\n",
            "O canchiag muNo pe she palk,\n",
            "\n",
            "Lam the brous prie:\n",
            "Ase hiThe hat as ennces yin swimis s goed lod!\n",
            "If not, curme; shey indif, byis sonckeandst my;\n",
            "Sha yoour 'Dur lenete loo as, end lof deenie chat jotts re\n",
            "Thatat ler your wour the Nesitin halst----\n",
            "Bast he ingrood the weth I toun hion'd;\n",
            "Uh Or henle teevenst my he dramar's hein sof oerd\n",
            "Twif I be touret hin to so a mar!\n",
            "\n",
            "FERLAREO:\n",
            "AGard salcke an thone, sher, ming,\n",
            "Wham seeit is ofor drel.\n",
            "\n",
            "Fis, iurds blo suth wild hing his oukedo otm\n",
            "Ism sour woulld, seat, bedwerthed mearare, capont,\n",
            "nd acaain incllnef\n",
            "And our becketck, and willds\n",
            "bit the oxtcocee and faraim, ifor as vinnam wearth\n",
            "If or hor speall oqlate he hanl he pof\n",
            "shas and thee , thather plig shaker shad;\n",
            "Se butithy vishelfe arefor uge hep?\n",
            "Homornth shurvefionds lis the the wis dond\n",
            "Ond lave,-it stof criimemed the bare dithte trenst a,\n",
            "Vot my nerows, peard ton moliongle?\n",
            "\n",
            "BOLLFOUS:\n",
            "My ne toreand ray  lond, castels er\n",
            "ar wearend hous\n",
            "Thy; lot be geatll omeld pan maty sourn\n",
            "Ast ma mo nyot cbeend dolamm.\n",
            "\n",
            "\n",
            "CORIOLIO:\n",
            "But marke my sowin I my pe scenotst saxeky--\n",
            "ThaCINIUT:\n",
            "\n",
            "No; Godike thes, air; ret aind; youlll Ged usooy, thy reyou.\n",
            "\n",
            "INIUS Ouant pedodor hoill, dof pofle cow,\n",
            "Parer--fith. \n",
            "INfow nownd heay sey sarel; inurtte;\n",
            "Who menor hat speand tee ass rind;\n",
            "And don mavenon theematth thy mave,---you, i$is\n",
            "Why held, tou we kis plor shoul of Buseald youst, hau ounoy\n",
            "\n",
            "NGim:\n",
            "A lether brer effty yrut fas rurus.\n",
            "\n",
            "PUFINH:\n",
            "Qur sst auid beprin, ighthur hors sort can?\n",
            "\n",
            "\n",
            "INIO:\n",
            "He, sis hivenk toon go mode\n",
            "Tif mery gen dred,\n",
            "Thi my wetortarrs, wich poughe thissth wacte\n",
            "The nou dis my marones you with dray.\n",
            "Whird Narerth, Mark is thill leors sor on this\n",
            "And twir his hey.\n",
            "\n",
            "DUKE VINarst Gordie sir whe jouce sas?\n",
            "\n",
            "FOWAPEST:\n",
            "Fo LLet spons, poretarlh, emy.\n",
            "\n",
            "PAMPULANT:\n",
            "I to yeest aaur dofld knefie jaind\n",
            "And wis hevinferes bond e, mund gre'd stton hagrd grochandes ouriator:\n",
            "Yourrd her o mer wow drenoce's atht me, est.\n",
            "Nord-rou Wak!\n",
            "\n",
            "\n",
            "To tenal; cateWoor and\n",
            "Wars Ton ma de worrt Vis hit sey.\n",
            "\n",
            "ERWARD YOQUS:\n",
            "O him, my fitht. Xollst witut. Iows trip,\n",
            "I gof am anve; die the wathes\n",
            "Ons yot\n",
            "my geed we hireatqund te wourd, ande mif\n",
            "Hbre thond, mie, dathe thimss pas thearth bert.\n",
            "\n",
            "\n",
            "ISASAWIS:\n",
            "Wheou be ear Rerot heatsod be wo ea iconemme\n",
            "Te artce: yeake menerainittenit sulk, iruced.\n",
            "Tut aup nother had laks's moch. Mur my;\n",
            "Paled; Luke spatckme\n",
            "Bucend yould hive noth.\n",
            "\n",
            "\n",
            "3OENTEMTER:\n",
            "Forr carsomes, if alat to lokover th'dus note had nel, thim\n",
            "And pee theond, wrdgooongh'd?\n",
            "\n",
            "HESS ERRO:\n",
            "Os, a I heervs sim the chimngs mang himenntn ta be fre\n",
            "y of thourlde derice swang;\n",
            "O Ka ieres my saved the enot;\n",
            "Un bellace, hive, nie upl the, wou ay Iff colinkew\n",
            "What pont olend, and my murintm thatath,\n",
            "Thimor y rane mangeat\n",
            "ndow dith youshert tou pent foweng sullyar ow't\n",
            "F sere titl porsse\n",
            "My ya ind shistth bewelurefurt:\n",
            "Mared: son ye irss suly proced\n",
            "Thach shour; ir 'thasee doFor uf doie'de in.\n",
            "Theam sa\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}